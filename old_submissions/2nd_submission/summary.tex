\section{Summary Functions}
\label{sec:summary_func}
%
%In this section, we introduce \emph{summary functions}, our solution to a key challenge in estimating the parameters of procedural material models: the need for a metric capable of comparing a pair of images based only on their overall appearance.
%
%\subsection{Appearance Matching via Summary Functions}
%
To solve the parameter estimation problem using Eq.~\eqref{eq:approx}, a key ingredient is the appearance-match relation.
Unfortunately, we cannot use simplistic image difference metrics such as the L2 or L1 norms.
This is because the features (bumps, scratches, flakes, yarns, etc.) in the images of real-world materials are generally misaligned, even when the two images represent the same material.
In procedural modeling, as shown in Figure \ref{fig:syn1}, with irregularities created differently using $\bz_1$ and $\bz_2$, the same procedural model parameters $\btheta$ can yield slightly different results $f(\btheta; \bz_1)$ and $f(\btheta, \bz_2)$.

\begin{figure}[t]
	\addtolength{\tabcolsep}{-5pt}
	\begin{tabular}{cccc}
		\includegraphics[width=0.24\columnwidth]{images/syn_comp/bump04rd1.jpg} &
		\includegraphics[width=0.24\columnwidth]{images/syn_comp/bump04rd2.jpg} &
		\includegraphics[width=0.24\columnwidth]{images/syn_comp/bump02rd1.jpg} &
		\includegraphics[width=0.24\columnwidth]{images/syn_comp/bump02rd2.jpg} \\
		(a1) & (a2) & (b1) & (b2)
	\end{tabular}
	\caption{\label{fig:syn1}
		Each pair of images among (a, b) are generated using identical model parameters $\btheta$ but different irregularities $\bz$. The pixel-wise L2 norm of the difference between these image pairs is large and not useful for estimating model parameters.
	}
\end{figure}


To address this challenge, we introduce a \emph{summary function}, which abstracts away the unimportant differences in the placement of the features, and summarizes the predicted and target images into smaller vectors whose similarity can be judged with simple metrics like L2 distance.

An image summary function (embedding) $\summ$ is a continuous function that maps an image of a material ($\target$ or $\synth$) into a vector in $\Reals^k$. An idealized summary function would have the property that
%
\begin{equation}
	\summ(f(\btheta_1, \bz_1)) = \summ(f(\btheta_2, \bz_2)) \ \Leftrightarrow \ \btheta_1 = \btheta_2.
\end{equation}
%
That is, applying the summary function would fully abstract away from the randomness $\bz$ and the difference between the two summary vectors would be entirely due to different material properties $\btheta$.

Practical summary functions will satisfy the above only approximately. However, a good practical summary function will embed images of the same material close to each other, and images of different materials further away from each other. Below we discuss several techniques for constructing summary functions.

\subsection{Our Summary Functions}
\label{ssec:example_summary_func}

\paragraph{Statistics of image bins.}
The simplest idea for a summary function is to subdivide the image into $k$ bins (regions) and compute the (scalar or RGB) mean of each region. For mostly isotropic materials, a flash photograph will normally lead to an approximately radially symmetric highlight, where we found concentric bins perform well. For anisotropic highlights (e.g. brushed metal), it makes more sense to define the bins as $k$ vertical or horizontal bands. Adding the standard deviation statistic per bin, in addition to mean, can significantly help with estimating frequencies and size of features such as bumps.

\paragraph{Fourier transforms.}
However, the simple per-bin statistics can be insufficient to precisely match frequency characteristics of material imperfections. A more complex and more powerful tool for summary function design is fast Fourier transforms, which captures frequencies explicitly. These can be applied to the whole image (as a 2D FFT), or part of it, e.g. as 1D FFTs over a subset of rows or columns. Note that automatic computation of derivatives is possible with the FFT, and supported by the PyTorch framework.

\paragraph{Neural summary function.}
Gatys et al. \cite{Gatys2015,Gatys2016} introduced the idea of using the features of an image classification neural network (usually VGG \cite{VGG}) as a descriptor $T_G$ of image texture (or style). Optimizing images to minimize the difference in $T_G$ (combined with other constraints) allowed Gatys et al. to produce impressive, state-of-the art results for parametric texture synthesis and style transfer between images. While further work  has introduced improvements \cite{Risser2017}, we find that the original version from Gatys et al. works already well in our case.

Aittala et al. \cite{Aittala2016} introduced this idea to capturing material parameter textures (albedo, roughness, normal and specular maps) of stationary materials. They optimized for a $256 \times 256$ stationary patch that matches the target image in various crops, using a combination of $T_G$ and a number of special Fourier-domain priors. In our case (for procedural materials), we find that the neural summary function $T_G$ works even more effectively; we can simply apply it to the entire target or simulated images (not requiring crops nor Fourier-domain priors).
%
Specifically, define the Gram matrix $G$ of a set of feature maps $F_1, \cdots, F_n$ as
\begin{equation}
	G = \mbox{mean}(F_i \cdot F_j),
\end{equation}
where the product $F_i \cdot F_j$ is element-wise. $T_G$ is defined as the concatenation of the flattened Gram matrices computed for the feature maps before each pooling operation in VGG19. Note that the size of the Gram matrices depends on the number of feature maps (channels), not their size; thus $T_G$ is independent of input image size.

