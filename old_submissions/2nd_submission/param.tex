\section{Bayesian Inference}
\label{sec:bayesian}
%
% It is often not possible to unambiguously determine the full parameter vector $\btheta$ from a single target image $\target$. Instead, there will commonly be a region of the parameter space that gives rise to images plausibly matching the target. This may also be due to over-completeness of the parameter space (different settings achieve same look).

% Using classical non-linear optimization can often give a good point estimate, matching the target image summary with low error. However, other estimates may also offer good matches to the target, either because the summary function is not powerful enough to distinguish them, or because the difference only shows up under other viewing or lighting conditions. A typical example is the mean scattering angle of the phase function, which is often hard to estimate from a single view; later we will show this and other examples of the similarity structures.

% We pose our problem in a Bayesian inference framework, capable of sampling the space of possible material parameters (the posterior distribution) using Markov chain Monte Carlo (MCMC) methods. This allows a user to both visually inspect the shape of the parameter space (or its low-dimensional embeddings), as well as click on specific samples and judge the appearance of the result.

In what follows, we first summarize the classical non-linear optimization approach to parameter estimation (and inverse problems in general), and its Bayesian formulation as a maximum a-posteriori (MAP) estimate. These approaches generally provide a reasonable single point estimate of the parameter vector. Next, we explain how the Hamiltonian Monte Carlo approach for Bayesian inference extends the classical approach.


\subsection{Point Estimates}
\label{ssec:point_sec}
%
\paragraph{Non-linear optimization.} Recall that our goal is to match the target $\target$, normally an image of a material sample under known illumination conditions. The model has unknown parameters $\btheta$. The \emph{forward evaluator} $f(\btheta, \bz)$ is available as a differentiable subroutine.  Given the availability of an appropriate summary function $\summ$, our goal is to find the value of $\btheta$ whose summary vector fits that of the target (measurement) $\target$ as closely as possible:
%
\begin{equation}
	\label{eq:opt}
	\argmin_{\btheta} \|\summ(f(\btheta, \bz)) - \summ(\target)\|^2.
\end{equation}
%
Recall that we are generally not interested in estimating $\bz$ that is meant to introduce irregularities.
The optimization in Eq.~\eqref{eq:opt} can be solved by standard non-linear optimization methods; regularization is commonly added to improve the stability.

\paragraph{Maximum a-posteriori estimation.} A technically similar but theoretically cleaner approach is to model the above in a probabilistic Bayesian framework, as the maximization of the posterior distribution. We treat the procedural model parameters $\btheta$ %as well as other quantities needed during estimation,
as random variables with corresponding probability distributions.

Specifically, we introduce a \emph{prior} probability distribution $p(\btheta)$ of the parameters, reflecting our pre-existing beliefs about the likelihood values of the unknown parameters. For example, in most material models, we know what range the albedo color and roughness coefficients of the material should typically be in.

Further, we model the $\approx$ operator from Eq.~\eqref{eq:approx} as an error distribution. Precisely, we postulate that the difference between the simulated image summary $\summ(f(\btheta, \bz))$ and the target %(measured)
image summary $\summ(\target)$ follows a known probability distribution.
In practice, we use a (multi-variate) normal distribution with zero mean and the covariance $\bsigma_e$:
%
\begin{equation}
	\summ(f(\btheta, \bz)) - \summ(\target) \sim \mathcal{N}(0, \bsigma_e).
\end{equation}
%
Our experiments indicate that this simple error distribution works well in practice, and we regard $\bsigma_e$ as a hyperparameter and set it manually.

We also have multiple options in handling the random vector $\bz$. While it is certainly theoretically possible to estimate it, we are not really interested in its values;  we find it simpler and more efficient to simply choose $\bz$ randomly, fix it, and assume it known during the process of estimating the ``interesting'' parameters $\btheta$.

Under these assumptions, according to the Bayes theorem, we can write down the posterior probability of parameters $\btheta$, conditional on the known values of $\target$ and $\bz$, as:
%
\begin{equation} \label{eq:posterior}
	p(\btheta | \target, \bz) \propto \mathcal{N}\left[\summ(f(\btheta, \bz)) - \summ(\target); 0, \bsigma_e\right] p(\btheta),
\end{equation}
%
%where the $G$ term is a $k$-dimensional Gaussian with zero mean and standard deviation vector $\bsigma_e$, and p(\btheta)$ is the prior on the material parameters.
where the right side does not need to be normalized. %as a pdf; the constant factor has no effect on parameter estimates.
For numerical stability, we compute the negative log posterior, viz. $-\log p(\btheta | \target, \bz)$, in practice.

In the maximum a-posteriori (MAP) framework we estimate the desired parameter values $\btheta$ as the maximum of the posterior pdf $p(\btheta | \target, \bz)$ given by Eq.~\eqref{eq:posterior}. This problem can be solved using many non-linear optimization algorithms.

% Note the close technical similarity to the classical optimization discussed above: taking the logarithm, the error Gaussian $G$ becomes a quadratic term. Note, if the priors were chosen to be Gaussian as well, the whole problem would turn into a standard non-linear least squares problem. In practice, because we do make use of non-Gaussian priors, and because our $\bsigma_e$ can be a function of other values, we have a general non-linear log-posterior; this is not an issue and can be handled effectively.

%In summary, in the MAP framework our goal becomes finding the maximum of the \emph{posterior probability distribution} of $\btheta$: this is the conditional probability $p(\btheta | \target, \bz)$ defined in Eq.~\eqref{eq:posterior}.


\subsection{Monte Carlo Sampling of the Posterior}
\label{ssec:bayesian}
%
Although the point estimate approach gives satisfactory results in many cases, it is not without problems. For example, since a perfect match between a procedural material and a photograph is generally impossible, it can be desired to have a set of imperfect matches for the user to choose from. Further, there could be an entire subset of the parameter space giving solutions of approximately equivalent fit under the target view and lighting; however, these may look quite different from each other in other configurations, and a user may want to explore those differences.

In this paper, we use the well-known technique of full Bayesian inference, sampling the posterior pdf defined in Eq.~\eqref{eq:posterior} using Markov-chain Monte Carlo techniques, specifically Hamiltonian Monte Carlo (HMC)~\cite{Betancourt2017}. While well explored in statistics and various scientific fields, to our knowledge, this technique has not been used for the inference of material parameters.

The goal of the sampling is to explore the posterior with many (typically thousands or more) samples, each of which represents a material parameter vector consistent with the target image. Plotting these samples projected into two dimensions (for a given pair of parameters) gives valuable insight into similarity structures. Furthermore, interactively clicking on samples and observing the predicted result can help a user to quickly zoom in on a preferred solution, which an automatic optimization algorithm is fundamentally incapable of.

Technically, HMC is similar to other Markov chain methods such as Metropolis-Hastings but is much more efficient for posterior sampling problems in cases where the derivatives of the model are available (always true in our case, as all our forward evaluators are implemented in PyTorch).
