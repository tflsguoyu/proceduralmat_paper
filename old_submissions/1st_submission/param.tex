\section{Bayesian inference for parameter estimation}

In this section, we first summarize the classical non-linear optimization approach to parameter estimation (and inverse problems in general), and its Bayesian formulation as a  maximum a-posteriori (MAP) estimate. These approaches normally provide a single point estimate of the parameter vector. Next, we explain how the Hamiltonian Monte Carlo approach for Bayesian inference extends the classical approach, resolving several important issues.


\subsection{Point estimates}

\paragraph{Non-linear optimization.} As above, assume our goal is to match the target image $\target$, normally the image of a material sample under known illumination conditions. The model has unknown parameters $\btheta$. The \emph{forward evaluator} $f(\btheta, \bz)$ is available as a differentiable subroutine.  Given the availability of an appropriate summary function $\summ$, our goal is to find the value of $\btheta$ whose summary vector fits the summary vector of the target (measurement) $\target$ as closely as possible:
\begin{equation} \label{eq:approx}
	\summ(f(\btheta, \bz)) \approx \summ(\target),
\end{equation}
where we did not specify the meaning of $\approx$ yet. One way to solve this problem in practice is to find an approximate solution for $\btheta$ by choosing a fixed $\bz$ at random and minimizing the nonlinear least squares objective function
\begin{equation}
	\argmin_{\btheta} \|\summ(f(\btheta, \bz)) - \summ(\target)\|^2.
\end{equation}
This can be accomplished by standard non-linear optimization methods like Levenberg-Marquardt or L-BFGS. Regularization is commonly added to improve the stability of this approach, often by adding a term like $\epsilon \|\btheta\|^2$ for a hand-picked constant $\epsilon$, or similar, usually ad-hoc terms.


\paragraph{Maximum a-posteriori estimation.} A technically similar but theoretically cleaner approach to the above problem is to model it in a probabilistic Bayesian framework, as the maximization of the posterior distribution. The key idea is to model the parameters $\btheta$, as well as other quantities needed during estimation, as random variables with corresponding probability distributions.

Specifically, we introduce a \emph{prior} probability distribution $p(\btheta)$ of the parameters, reflecting our pre-existing beliefs about the likely values of the unknown parameters. For example, in most material models, we know what range the color and roughness coefficients of the material should typically be in, and we know that shading normals tend to point up, with some variation.

Furthermore, we model the $\approx$ operator from eq. \ref{eq:approx} as an error distribution. More precisely, we postulate that the difference between the simulated image summary $\summ(f(\btheta, \bz))$ and the target (measured) image summary $\summ(\target)$ follows a known probability distribution; for example, we can use a Gaussian (normal) distribution with zero mean and a $k$-dimensional error vector $\bsigma_e$:
\begin{equation}
	\summ(f(\btheta, \bz)) - \summ(\target) \sim \mathcal N(0, \bsigma_e).
\end{equation}
We find that this error distribution works well in practice, and set $\bsigma_e$ by hand, even though it could be estimated together with the material parameters.

We also have multiple options in handling the random vector $\bz$. While it is certainly theoretically possible to estimate it, we are not really interested in its values;  we find it simpler and more efficient to simply choose $\bz$ randomly, fix it, and assume it known during the process of estimating the ``interesting'' parameters $\btheta$.

Under these assumptions, and after applying the Bayes theorem, we can write down the posterior probability of parameters $\btheta$, conditional on the known values of $\target$ and $\bz$, as:
\begin{equation} \label{eq:posterior}
	p(\btheta | \target, \bz) \propto G(\summ(f(\btheta, \bz)) - \summ(\target); 0, \bsigma_e) \ p(\btheta),
\end{equation}
where the $G$ term is a $k$-dimensional Gaussian with zero mean and standard deviation vector $\bsigma_e$, and $p(\btheta)$ is the prior on the material parameters.
The right side of the equation does not need to be normalized as a pdf; the constant factor is not important in the following.

In the maximum a-posteriori (MAP) framework we estimate the desired parameter values $\btheta$ as the maximum of the posterior pdf $p(\btheta | \target, \bz)$. This problem can be solved using a number of non-linear optimizers. Note the close technical similarity to the classical optimization discussed above: taking the logarithm, the error Gaussian $G$ becomes a quadratic term. Note, if the priors were chosen to be Gaussian as well, the whole problem would turn into a standard non-linear least squares problem. In practice, because we do make use of non-Gaussian priors, and because our $\bsigma_e$ can be a function of other values, we have a general non-linear log-posterior; this is not an issue and can be handled effectively.

In summary, unlike the classical optimization approach, where the goal is to find a solution to the approximate equations $f(\btheta, \bz) \approx \target$ or (in the summary function framework) $\summ(f(\btheta, \bz)) \approx \summ(\target)$, in the MAP framework our goal becomes finding the maximum of the \emph{posterior probability distribution} of $\btheta$: this is the conditional probability $p(\btheta | \target, \bz)$ defined in eq. \ref{eq:posterior}.



\subsection{Monte Carlo sampling of the posterior}

The point estimate approach gives satisfactory results in some cases, but also raises several questions. For example, will the optimization converge? Assuming the algorithm converges, is the resulting minimum global or just local? If local, how many other local minima are there? Even if the minimum found is global, does it truly give the best solution to the original problem, or is the solution biased by our choice of priors or summary function?

Furthermore, there could be an entire subset of the parameter space giving solutions of approximately equivalent quality; in other words, the solution space can exhibit a \emph{similarity structure}. This problem becomes important with more powerful models containing more parameters. Frequently, this issue would historically lead computer graphics researchers to avoid additional power in models, instead preferring simple BRDFs.

In this paper, we use the well-known technique of full Bayesian inference, sampling the posterior pdf defined in eq. \ref{eq:posterior} using Markov chain techniques, specifically Hamiltonian Monte Carlo \cite{Betancourt2017}. While well explored in statistics and various scientific fields, to our knowledge this technique has not been used for material capture in computer graphics.

The goal of the sampling is to explore the posterior with many (typically thousands or more) samples, each of which represents a material parameter vector consistent with the target image. Plotting these samples projected into two dimensions (for a given pair of parameters) gives valuable insight into similarity structures. Furthermore, interactively clicking on samples and observing the predicted result can help a user to quickly zoom in on a preferred solution, which an automatic optimization algorithm is fundamentally incapable of.

Technically, HMC is similar to Markov chain methods such as Metropolis-Hastings, but is much more efficient for posterior sampling problems in cases where the derivatives of the model are available (always true in our case, as all our forward evaluators are implemented in \torch). We use the Stan framework to define the top-level model (including parameter priors $p(\btheta)$ and the error vector $\bsigma_e$), and to carry out the posterior sampling.



% - the approximation error inherent in the problem

% - an uninformative prior (one where all parameter values are equally likely)
% - this gives rise to a \emph{maximum likelihood estimate}
