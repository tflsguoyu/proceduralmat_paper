\section{Related work}

Here we cover previous work on material parameter estimation in computer graphics, as well as on Hamiltonian Monte Carlo methods in Bayesian inference.

\paragraph{Per-pixel SVBRDF capture.} A large amount of previous work in computer graphics focuses on acquisition of materials from physical measurements. The methods generally observe the material sample with a fixed camera position, and solve for the parameters of a BRDF model such as diffuse albedo, roughness (glossiness) and surface normal. They differ in the number of light patterns required and their type; the patterns used include moving linear light \cite{Gardner2003}, Gray code patterns \cite{Francken2009} and spherical harmonic illumination \cite{Ghosh2009}. In these approaches, the model and its optimization are specific to the light patterns and the optical setup of the method, as general non-linear optimization was historically deemed too inefficient and not robust enough.

More recently, Aittala et al. \shortcite{Aittala2013} captured per-pixel SVBRDF data using Fourier patterns projected using an LCD screen; their algorithm used a fairly general, differentiable forward evaluation model, which was inverted in a maximum a-posteriori (MAP) framework. In practice, this was done using a standard non-linear least-squares optimizer with well-chosen priors, showing that a general optimization approach with a differentiable forward model can be successful with carefully chosen priors and initialization.

Later work by Aittala et al. \shortcite{Aittala2015,Aittala2016} found per-pixel parameters of stationary spatially-varying SVBRDFs from two-shot and one-shot flash-lit photographs, respectively. In the latter case, the approach used a neural texture descriptor as a more advanced loss function without requiring per-pixel alignment; this is related to our summary function concept.

Recent methods by Deschaintre et al. \shortcite{Deschaintre2018} and Li et al. \shortcite{Li2018} have been able to capture non-stationary SVBRDFs from a single flash photograph by training an end-to-end deep convolutional network. All of these approaches estimate per-pixel parameters of the microfacet-diffuse-normal model, and are not obviously applicable to estimation of global model parameters, nor to more advanced optical models used in some of our examples (significant anisotropy, layering or scattering).

\paragraph{Global parameter estimation.} Focus on estimating the global parameters of material models (that is, not per-pixel estimates) has been relatively rare in previous material capture efforts; however, there are a few exceptions. The dual-scale glossy parameter estimation work of Wang et al. \shortcite{Wang2011} finds, under step-edge lighting, the parameters of a bumpy surface model consisting of a heightfield constructed from a Gaussian noise power spectrum and global microfacet material parameters. Their results provide impressive accuracy, but their solution is highly specialized for this material model and illumination. This problem is closely related to our ``bumpy'' example, which essentially replicates their result (under flash illumination), but using a more general framework.

Several approaches for appearance rendering of cloth have used micro-CT scans to model the material at the microscopic fiber level. However, the optical properties of the fibers (e.g. roughness, scattering albedo) are not available from the scans and have to be chosen separately. Zhao et al. \shortcite{Zhao2011} use a simple but effective trick of matching the mean and standard deviation (in RGB) of the pixels in a well-chosen area of the target and simulated image. Khungurn et al. \shortcite{Khungurn2015} have extended this approach with a differentiable volumetric renderer, combined with a stochastic gradient descent; however, their method is still specific to fiber-level modeling of cloth.


\paragraph{Bayesian inference.} A variety of methods used across the sciences are Bayesian in nature; in this paper, we specifically explore Bayesian inference for parameter estimation through Markov chain Monte Carlo sampling of the posterior distribution.

Hamiltonian Monte Carlo (HMC) \cite{Neal2012,Betancourt2017} is an algorithm for sampling a multi-dimensional continuous probability distribution (pdf), given just a piece of code that evaluates the log pdf and its gradient, the method can effectively explore the space, sampling points with probability proportional to the pdf. The gradient information generally leads to much more efficient sampling than with simpler methods such as Metropolis-Hastings.

STAN \cite{Stan} is a software package for Bayesian inference, allowing a user to specify a statistical forward model and parameter, whose posterior can then be sampled using HMC or maximized using a non-linear optimizer (L-BFGS). We use Stan in our results; however, our forward evaluation models are largely implemented as custom Stan functions, using the \torch C++ library. This approach provides us with automatic differentiation and GPU acceleration, while being powerful enough to express a variety of material models.


